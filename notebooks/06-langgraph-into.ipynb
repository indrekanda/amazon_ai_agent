{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e0fda1",
   "metadata": {},
   "source": [
    "### **LangGraph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6d9e2",
   "metadata": {},
   "source": [
    "**State** (an object) aka short term memory: The State typically defines the data structure that will be passed between nodes in the graph. It can be understood as the shared memory / context that each step of the workflow can read from and write to.\n",
    "\n",
    "**Workflow**: StateGraph workflow is a powerful way to orchestrate multi-step processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da564190",
   "metadata": {},
   "source": [
    "#### Simple single-node graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State's pydantic model / shcema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class State(BaseModel):\n",
    "    message: str\n",
    "    answer: str=\"\"\n",
    "    vibe: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd78ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A node is an action (a function that takes in a state and returns a dict)\n",
    "def append_vibes_to_query(state: State) -> dict:\n",
    "    \"\"\"Generate value for state key 'answer' by appending the vibe to the message\"\"\"\n",
    "    return {\"answer\": f\"{state.message} {state.vibe}\"}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ba7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple graph - workflow (for now empty)\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "#  Initializes a new graph-based workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define nodes (functions)\n",
    "workflow.add_node(\"append_vibes_to_query\", append_vibes_to_query)\n",
    "\n",
    "# Add edges (add START-entry point and END nodes)\n",
    "# START is a special constant indicating the beginning of the graph. \n",
    "workflow.add_edge(START, \"append_vibes_to_query\")\n",
    "# END is a special constant indicating the end of the graph. \n",
    "workflow.add_edge(\"append_vibes_to_query\", END)\n",
    "\n",
    "# compiles the defined workflow into an executable graph\n",
    "# Run the graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate th state\n",
    "initial_state = {\n",
    "    \"message\": \"Give me some vibes!\", \n",
    "    \"vibe\": \"I'm feeling like a badass today!\"\n",
    "    }\n",
    "\n",
    "# Invoke the graph (compute the state, the results is a dict of the from we defined in pydantic model)\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "# Display the result - we evolved the state with adding valua to a key \"answer\"\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f75523",
   "metadata": {},
   "source": [
    "#### Conditional graph (a base for router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebeff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pydantic model for state\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class State(BaseModel):\n",
    "    message: str\n",
    "    answer: str=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node\n",
    "def append_vibes_to_query(state: State) -> dict:\n",
    "    \"\"\"Generate value for state key 'answer' by overwriting the existing value\"\"\"\n",
    "    return {\"answer\": \"I am here to add some vibes.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another node (router)\n",
    "from typing import Literal\n",
    "import random\n",
    "def router(state: State) -> Literal[\"append_vibe_1\", \"append_vibe_2\", \"append_vibe_3\"]:\n",
    "    \"\"\"Route the state to the appropriate node\"\"\"\n",
    "    vibes = [\"append_vibe_1\", \"append_vibe_2\", \"append_vibe_3\"]\n",
    "    vibe_path = random.choice(vibes)\n",
    "    return vibe_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node to route to\n",
    "# appends vibe to the existing answer\n",
    "def append_vibe_1(state: State) -> dict:\n",
    "    vibe = \"I'm feeling like a badass today!\"\n",
    "    return {\"answer\": f\"{state.answer} {state.message} {vibe}\"}\n",
    "\n",
    "def append_vibe_2(state: State) -> dict:\n",
    "    vibe = \"I'm feeling chill today!\"\n",
    "    return {\"answer\": f\"{state.answer} {state.message} {vibe}\"}\n",
    "\n",
    "def append_vibe_3(state: State) -> dict:\n",
    "    vibe = \"I'm sick today!\"\n",
    "    return {\"answer\": f\"{state.answer} {state.message} {vibe}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grpah\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# a node (action or computation)\n",
    "workflow.add_node(\"append_vibes_to_query\", append_vibes_to_query)\n",
    "workflow.add_node(\"append_vibe_1\", append_vibe_1)\n",
    "workflow.add_node(\"append_vibe_2\", append_vibe_2)\n",
    "workflow.add_node(\"append_vibe_3\", append_vibe_3)\n",
    "\n",
    "# edges (from - to)\n",
    "# Start with append_vibes_to_query\n",
    "workflow.add_edge(START, \"append_vibes_to_query\")\n",
    "# Route to the appropriate node\n",
    "workflow.add_conditional_edges(\"append_vibes_to_query\", router)\n",
    "# End with the appropriate node\n",
    "workflow.add_edge(\"append_vibe_1\", END)\n",
    "workflow.add_edge(\"append_vibe_2\", END)\n",
    "workflow.add_edge(\"append_vibe_3\", END)\n",
    "\n",
    "# Run the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the graph\n",
    "# Initiate th state\n",
    "initial_state = {\"message\": \"Give me some vibes!\"}\n",
    "result = graph.invoke(initial_state)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224a597",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6afdd",
   "metadata": {},
   "source": [
    "### **Agent graph (with LLM decision)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\"message\": \"Give me some vibes!\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab807c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced18b8",
   "metadata": {},
   "source": [
    "A tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tool to be used in a graph\n",
    "# Write properly with type hints and description to be used as a tool by LLM\n",
    "def append_vibes(query: str, vibe: str) -> str:\n",
    "    \"\"\"Takes in a query and a vibe and returns a string with the query and vibe appended.\n",
    "\n",
    "    Args:\n",
    "        query: The query to append the vibe to.\n",
    "        vibe: The vibe to append to the query.\n",
    "\n",
    "    Returns:\n",
    "        A string with the query and vibe appended.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"{query} {vibe}\")\n",
    "    return f\"{query} {vibe}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d998ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------------------------------------------------------\n",
    "# # Explanation\n",
    "# #\n",
    "# # Prepare the function to be used in a graph by LLM\n",
    "# # 1. stringify the function \n",
    "# # 2. parse the function to extract values (json schema format)\n",
    "# # 3. inject the description to the prompt\n",
    "\n",
    "# print(\"Stringify the function\")\n",
    "# # inspect.getsource() with globals() vs without\n",
    "# # globals() approach is more flexible (looks up for a string in globals, not necessarily the function object).\n",
    "# function_string = inspect.getsource(globals()[\"append_vibes\"])\n",
    "# # function_string = inspect.getsource(append_vibes)\n",
    "# print(function_string)\n",
    "\n",
    "# print(\"--------------------------------\")\n",
    "# print(\"Parse the function\")\n",
    "# # Parse the function (at the bottom of the notebook)\n",
    "# result = parse_function_definition(function_string)\n",
    "# result\n",
    "# # # ---------------------------------------------------------------------------\n",
    "# # Why This Structure?\n",
    "# # This follows the JSON Schema format, which is commonly used for API documentation and validation:\n",
    "# # parameters represents the overall parameter schema\n",
    "# # properties contains the individual parameter definitions\n",
    "# # This structure allows for easy validation and documentation generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b2e67",
   "metadata": {},
   "source": [
    "Pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a state schema (a bit more complex)\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph\n",
    "from operator import add\n",
    "\n",
    "class ToolCall(BaseModel):\n",
    "    name: str  # Eg.: append_vibes\n",
    "    arguments: dict  # Eg.: {\"query\": \"How are you?\", \"vibe\": \"Feeling good\"}\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    answer: str\n",
    "    tool_calls: List[ToolCall] = Field(default_factory=list)\n",
    "\n",
    "# add is a reducer, it means that if any node is returning dict or other data object \n",
    "# with a key \"messages\", it will be added to the messages list\n",
    "# keys that do not have add, will be overwritten\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    message: str = \"\"\n",
    "    iteration: int = Field(default=0)\n",
    "    answer: str = \"\"\n",
    "    available_tools: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[ToolCall] = Field(default_factory=list) # ToolCall is a pydantic model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1715754",
   "metadata": {},
   "source": [
    "Nodes: LLM node (agent node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd6b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent node\n",
    "from jinja2 import Template\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# tojson  >>> converts dict to json string\n",
    "# - Once you have the vibe from the tool, return it as an answer, you can reformulate it a bit.\n",
    "\n",
    "def agent_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are a assistant that is generating vibes for a user.\n",
    "\n",
    "You will be given a selection of tools you can use to add vibes to a user's query.\n",
    "\n",
    "<Available tools>\n",
    "{{ available_tools | tojson }}\n",
    "</Available tools>\n",
    "\n",
    "When you need to use a tool, format your response as:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": \"tool_name\", \"arguments\": {...}}\n",
    "</tool_call>\n",
    "\n",
    "Instructions:\n",
    "- You need to use the tools to add vibes to the user's query.\n",
    "- Add a random vibe to the user's query.\n",
    "\"\"\"\n",
    "   # 1. Make a template of the prompt\n",
    "   # Jinja2 template to inject the available_tools to the prompt\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   # 2. Inject the available_tools to the prompt\n",
    "   # available_tools is a list of tool descriptions & be passed to the prompt {{ }}\n",
    "   # In prompt it will be rendered to a json string\n",
    "   # tolls are extracted from tool_descriptions = get_tool_descriptions_from_node(tool_node)\n",
    "   prompt = template.render(available_tools=state.available_tools)\n",
    "\n",
    "   # 3. Use the instructor to enforce structured output; initialize the client\n",
    "   # The instructor.from_openai() function \"patches\" or wraps the standard OpenAI client. \n",
    "   # This patching process enhances the client with Instructor's capabilities, primarily \n",
    "   # its ability to enforce structured output from large language models (LLMs).\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   # 4. Use the client to create a response\n",
    "   # response is a pydantic model, AgentResponse\n",
    "   # raw_response is a raw response from the LLM\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=AgentResponse,\n",
    "        messages=[\n",
    "           {\"role\": \"system\", \"content\": prompt}, # promt with available tools\n",
    "           {\"role\": \"user\", \"content\": state.message}], # initial message, or user message\n",
    "        temperature=0.5,\n",
    "        #temperature=1.5,\n",
    "   )\n",
    "\n",
    "   # 5. Create an ai message (AIMessage is a langchain message type; like open AI has \"user\", \"system\", \"tool\")\n",
    "   # IF: If there are tool calls, collect tool calls & their arguments in a list\n",
    "   # ELSE: If there are no tool calls, just return the regular LLM response\n",
    "   if response.tool_calls:\n",
    "      tool_calls = []\n",
    "      for i, tc in enumerate(response.tool_calls): # collect tool call info\n",
    "         tool_calls.append({\n",
    "               \"id\": f\"call_{i}\",\n",
    "               \"name\": tc.name,\n",
    "               \"args\": tc.arguments\n",
    "         })\n",
    "      ai_message = AIMessage( \n",
    "         content=response.answer, # answer is a string, regular LLM response\n",
    "         tool_calls=tool_calls    # tool calls is a list of tool calls & their arguments\n",
    "         )\n",
    "   else: \n",
    "      ai_message = AIMessage( \n",
    "         content=response.answer, # just the answer, no tool calls\n",
    "      )\n",
    "\n",
    "   return {\n",
    "      \"messages\": [ai_message],\n",
    "      \"tool_calls\": response.tool_calls\n",
    "   }\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565ce5d",
   "metadata": {},
   "source": [
    "Node: router (tools selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a state has tools to be executed, we go to the tool node\n",
    "# If not, we end the workflow\n",
    "def tool_router(state: State) -> str:\n",
    "    \"\"\"Decide whether to continue or end\"\"\"\n",
    "    \n",
    "    if len(state.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db5243",
   "metadata": {},
   "source": [
    "A graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# 1. Initialize a graph\n",
    "# Buil a graph with a tool. Graph name is \"workflow\"\n",
    "# This initializes a new graph-based workflow. \n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 2. Prepare the tools and info about them (tool_descriptions)\n",
    "# Available tools & dedicated langgraph node that executes tools\n",
    "tools = [append_vibes]\n",
    "# tool node will try to envoke / execute the tool\n",
    "tool_node = ToolNode(tools)\n",
    "display(tool_node)\n",
    "\n",
    "# ToolNode has all the tools, we exctract descriptions from it\n",
    "# We use wrapper function to extract the tool description, it is defined at the bottom of the notebook\n",
    "# Wrapper function loops through the tools and extracts the tool description usin parse_function_definition\n",
    "# Returns a list of tool descriptions; which will be used to inject to the State at the start as initial_state\n",
    "tool_descriptions = get_tool_descriptions_from_node(tool_node)\n",
    "display(tool_descriptions)\n",
    "\n",
    "# 3. Add nodes to the graph\n",
    "# Nodes\n",
    "# Agent node is a function defined above\n",
    "workflow.add_node(\"agent_node\", agent_node)\n",
    "workflow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# 4. Add edges to the graph\n",
    "# Edges\n",
    "workflow.add_edge(START, \"agent_node\")\n",
    "# Conditional edge; \n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_node\", # from agent node\n",
    "    tool_router,  # use router to decide where to go next\n",
    "    {\"tools\": \"tool_node\",  # true branch (execute tools)\n",
    "     \"end\": END}            # false branch\n",
    ")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62716fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87793166",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Invoke the graph with the initial state, which has the message & available tools\n",
    "initial_state = {\n",
    "    \"message\": \"Give me some vibes!\",\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "\n",
    "# # More proper way to define the initial state, as pydantic model expects\n",
    "# initial_state = State(\n",
    "#     **initial_state\n",
    "# )\n",
    "\n",
    "# # or\n",
    "# initial_state = State(\n",
    "#     message=\"Give me some vibes!\",\n",
    "#     available_tools=tool_descriptions\n",
    "# )\n",
    "\n",
    "# LLM cames up with the answer; \n",
    "# the warning is because the graph expects State object, not simple dict\n",
    "# Here's what's happening:\n",
    "# agent_node - Makes the LLM decision and creates tool calls\n",
    "# tool_node - Executes the append_vibes function\n",
    "# append_vibes function - This is where the print happens (answer is printed)\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5eb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in result[\"messages\"]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca7a0b6",
   "metadata": {},
   "source": [
    "### **Agent graph with Loopback from Tools**\n",
    "React patern (reason and act: loops) - 1:10\n",
    "\n",
    "React agent implemented without LangChain, but using LangGraph and Instructor\n",
    "\n",
    "https://maven.com/swirl-ai/end-to-end-ai-engineering/1/syllabus/modules/3d5e45?item=9afl7815qpu\n",
    "https://github.com/swirl-ai/sprint-03-ai-engineering-bootcamp/blob/main/notebooks/05-LangGraph-intro.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models (for structured outputs)\n",
    "\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph\n",
    "from operator import add\n",
    "\n",
    "# ToolCalls\n",
    "class ToolCall(BaseModel):\n",
    "    name: str  # Eg.: append_vibes\n",
    "    arguments: dict  # Eg.: {\"query\": \"How are you?\", \"vibe\": \"Feeling good\"}\n",
    "\n",
    "# AgentResponse\n",
    "class AgentResponse(BaseModel):\n",
    "    answer: str\n",
    "    tool_calls: List[ToolCall] = Field(default_factory=list)\n",
    "\n",
    "# State (short term memory: all messages and tool calls)\n",
    "# NEW: stop condion (we will use iteration) it was before, just highlithing\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    message: str = \"\"\n",
    "    iteration: int = Field(default=0)\n",
    "    answer: str = \"\"\n",
    "    available_tools: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[ToolCall] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tool (without print)\n",
    "def append_vibes(query: str, vibe: str) -> str:\n",
    "    \"\"\"Takes in a query and a vibe and returns a string with the query and vibe appended.\n",
    "\n",
    "    Args:\n",
    "        query: The query to append the vibe to.\n",
    "        vibe: The vibe to append to the query.\n",
    "\n",
    "    Returns:\n",
    "        A string with the query and vibe appended.\n",
    "    \"\"\"\n",
    "    return f\"{query} {vibe}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent (with short term memory)\n",
    "\n",
    "from jinja2 import Template\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "def agent_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are a assistant that is generating vibes for a user.\n",
    "\n",
    "You will be given a selection of tools you can use to add vibes to a user's query.\n",
    "\n",
    "<Available tools>\n",
    "{{ available_tools | tojson }}\n",
    "</Available tools>\n",
    "\n",
    "When you need to use a tool, format your response as:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": \"tool_name\", \"arguments\": {...}}\n",
    "</tool_call>\n",
    "\n",
    "Instructions:\n",
    "- You need to use the tools to add vibes to the user's query.\n",
    "- Add a random vibe to the user's query.\n",
    "- Once you have the vibe from the tool, return it as an answer, you can reformulate it a bit.\n",
    "\"\"\"\n",
    "   # 1. Make a template of the prompt (jinja2 template to inject variables)\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   # 2. Inject the available_tools to the prompt\n",
    "   prompt = template.render(available_tools=state.available_tools)\n",
    "   \n",
    "   # 3. NEW: History of messages (state messages converted to open ai json messages format)\n",
    "   messages = state.messages\n",
    "   conversation = []\n",
    "   for message in messages:\n",
    "      conversation.append(lc_messages_to_regular_messages(message)) # result is a dict{role, content}\n",
    "   \n",
    "   \n",
    "   # 3. Use the instructor to enforce structured output; initialize the client\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   # 4. Use the client to create a response\n",
    "   # NEW: convert from langchain message types to open ai json messages format\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=AgentResponse,\n",
    "        messages=[\n",
    "           {\"role\": \"system\", \"content\": prompt}, \n",
    "           *conversation], # NEW: instead of initial message, use history of messages = SHORT TERM MEMORY\n",
    "        #  [dict{role, content}, dict{role, content}] this is conversation format\n",
    "        temperature=1.5,\n",
    "   )\n",
    "\n",
    "   # 5. Create an ai message (langchain message type)\n",
    "   # If tool calls, returns LLM response & tools, if no tools - returns response\n",
    "   # NEW: Add history of messages to the state (messages)\n",
    "   if response.tool_calls:\n",
    "      tool_calls = [] # list of tool calls (dicts: id, name, args)\n",
    "      for i, tc in enumerate(response.tool_calls): \n",
    "         tool_calls.append({\n",
    "               \"id\": f\"call_{i}\",\n",
    "               \"name\": tc.name,\n",
    "               \"args\": tc.arguments\n",
    "         })\n",
    "      ai_message = AIMessage( \n",
    "         content=response.answer, # LLM response (str)\n",
    "         tool_calls=tool_calls    # tool_calls (list of dicts)\n",
    "         )\n",
    "   else: \n",
    "      ai_message = AIMessage( \n",
    "         content=response.answer, # LLM response (str)\n",
    "      )\n",
    "\n",
    "   return {\n",
    "      \"messages\": [ai_message],\n",
    "      \"tool_calls\": response.tool_calls, \n",
    "      \"iteration\": state.iteration + 1, # NEW: iteration (number of iterations)\n",
    "      \"answer\": response.answer, # NEW: answer (str)\n",
    "   }\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: additional conditions to run the agent only once\n",
    "# Tool router: if there are tool calls, go to the tool node, otherwise end the workflow\n",
    "def tool_router(state: State) -> str:\n",
    "    \"\"\"Decide whether to continue or end\"\"\"\n",
    "    if state.iteration > 1: # run only once\n",
    "        return \"end\"\n",
    "    elif len(state.tool_calls) > 0: \n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90417b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A graph\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "tools = [append_vibes]\n",
    "tool_node = ToolNode(tools)\n",
    "tool_descriptions=get_tool_descriptions_from_node(tool_node)\n",
    "\n",
    "workflow.add_node(\"agent_node\", agent_node)\n",
    "workflow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent_node\")\n",
    "workflow.add_conditional_edges(\n",
    "   \"agent_node\",\n",
    "   tool_router,\n",
    "   {\n",
    "      \"tools\": \"tool_node\", # keys are returned by the tool_router\n",
    "      \"end\": END            # keys are returned by the tool_router\n",
    "   }\n",
    ")\n",
    "\n",
    "# This makes the graph ReAct type\n",
    "workflow.add_edge(\"tool_node\", \"agent_node\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute\n",
    "# 1. Initial state\n",
    "initial_state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Give me some vibes!\"}], # we start with messages because we want history\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "\n",
    "# 2. Execute the graph\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554315b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2f887",
   "metadata": {},
   "source": [
    "### **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import Dict, Any\n",
    "\n",
    "def parse_function_definition(function_def: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse a function definition string to extract metadata including type hints.\"\"\"\n",
    "    result = {\n",
    "        \"name\": \"\",\n",
    "        \"description\": \"\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
    "        \"required\": [],\n",
    "        \"returns\": {\"type\": \"string\", \"description\": \"\"}\n",
    "    }\n",
    "    \n",
    "    # Parse the function using AST\n",
    "    tree = ast.parse(function_def.strip())\n",
    "    if not tree.body or not isinstance(tree.body[0], ast.FunctionDef):\n",
    "        return result\n",
    "        \n",
    "    func = tree.body[0]\n",
    "    result[\"name\"] = func.name\n",
    "    \n",
    "    # Extract docstring\n",
    "    docstring = ast.get_docstring(func) or \"\"\n",
    "    if docstring:\n",
    "        # Extract description (first line/paragraph)\n",
    "        desc_end = docstring.find('\\n\\n') if '\\n\\n' in docstring else docstring.find('\\nArgs:')\n",
    "        desc_end = desc_end if desc_end > 0 else docstring.find('\\nParameters:')\n",
    "        result[\"description\"] = docstring[:desc_end].strip() if desc_end > 0 else docstring.strip()\n",
    "        \n",
    "        # Parse parameter descriptions\n",
    "        param_descs = parse_docstring_params(docstring)\n",
    "        \n",
    "        # Extract return description\n",
    "        if \"Returns:\" in docstring:\n",
    "            result[\"returns\"][\"description\"] = docstring.split(\"Returns:\")[1].strip().split('\\n')[0]\n",
    "    \n",
    "    # Extract parameters with type hints\n",
    "    args = func.args\n",
    "    defaults = args.defaults\n",
    "    num_args = len(args.args)\n",
    "    num_defaults = len(defaults)\n",
    "    \n",
    "    for i, arg in enumerate(args.args):\n",
    "        if arg.arg == 'self':\n",
    "            continue\n",
    "            \n",
    "        param_info = {\n",
    "            \"type\": get_type_from_annotation(arg.annotation) if arg.annotation else \"string\",\n",
    "            \"description\": param_descs.get(arg.arg, \"\")\n",
    "        }\n",
    "        \n",
    "        # Check for default value\n",
    "        default_idx = i - (num_args - num_defaults)\n",
    "        if default_idx >= 0:\n",
    "            param_info[\"default\"] = ast.literal_eval(ast.unparse(defaults[default_idx]))\n",
    "        else:\n",
    "            result[\"required\"].append(arg.arg)\n",
    "        \n",
    "        result[\"parameters\"][\"properties\"][arg.arg] = param_info\n",
    "    \n",
    "    # Extract return type\n",
    "    if func.returns:\n",
    "        result[\"returns\"][\"type\"] = get_type_from_annotation(func.returns)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_type_from_annotation(annotation) -> str:\n",
    "    \"\"\"Convert AST annotation to type string.\"\"\"\n",
    "    if not annotation:\n",
    "        return \"string\"\n",
    "    \n",
    "    type_map = {\n",
    "        'str': 'string',\n",
    "        'int': 'integer', \n",
    "        'float': 'number',\n",
    "        'bool': 'boolean',\n",
    "        'list': 'array',\n",
    "        'dict': 'object',\n",
    "        'List': 'array',\n",
    "        'Dict': 'object'\n",
    "    }\n",
    "    \n",
    "    if isinstance(annotation, ast.Name):\n",
    "        return type_map.get(annotation.id, annotation.id)\n",
    "    elif isinstance(annotation, ast.Subscript) and isinstance(annotation.value, ast.Name):\n",
    "        base_type = annotation.value.id\n",
    "        return type_map.get(base_type, base_type.lower())\n",
    "    \n",
    "    return \"string\"\n",
    "\n",
    "\n",
    "def parse_docstring_params(docstring: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract parameter descriptions from docstring (handles both Args: and Parameters: formats).\"\"\"\n",
    "    params = {}\n",
    "    lines = docstring.split('\\n')\n",
    "    in_params = False\n",
    "    current_param = None\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check for parameter section start\n",
    "        if stripped in ['Args:', 'Arguments:', 'Parameters:', 'Params:']:\n",
    "            in_params = True\n",
    "            current_param = None\n",
    "        elif stripped.startswith('Returns:') or stripped.startswith('Raises:'):\n",
    "            in_params = False\n",
    "        elif in_params:\n",
    "            # Parse parameter line (handles \"param: desc\" and \"- param: desc\" formats)\n",
    "            if ':' in stripped and (stripped[0].isalpha() or stripped.startswith(('-', '*'))):\n",
    "                param_name = stripped.lstrip('- *').split(':')[0].strip()\n",
    "                param_desc = ':'.join(stripped.lstrip('- *').split(':')[1:]).strip()\n",
    "                params[param_name] = param_desc\n",
    "                current_param = param_name\n",
    "            elif current_param and stripped:\n",
    "                # Continuation of previous parameter description\n",
    "                params[current_param] += ' ' + stripped\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "def get_tool_descriptions_from_node(tool_node):\n",
    "    \"\"\"Loop all tools and extract tool descriptions from the ToolNode object.\"\"\"\n",
    "    descriptions = []\n",
    "    \n",
    "    if hasattr(tool_node, 'tools_by_name'):\n",
    "        tools_by_name = tool_node.tools_by_name\n",
    "        \n",
    "        for tool_name, tool in tools_by_name.items():\n",
    "            function_string = inspect.getsource(globals()[tool_name])\n",
    "            result = parse_function_definition(function_string)\n",
    "\n",
    "            if result:\n",
    "                descriptions.append(result)\n",
    "    \n",
    "    return descriptions if descriptions else \"Could not extract tool descriptions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "import json\n",
    "\n",
    "def lc_messages_to_regular_messages(msg):\n",
    "    \"\"\"\n",
    "    Convert various message types to oppen ai compatible format\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(msg, dict):\n",
    "        \n",
    "        if msg.get(\"role\") == \"user\":\n",
    "            return {\"role\": \"user\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"assistant\":\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"tool\":\n",
    "            return {\n",
    "                \"role\": \"tool\", \n",
    "                \"content\": msg[\"content\"], \n",
    "                \"tool_call_id\": msg.get(\"tool_call_id\")\n",
    "            }\n",
    "        \n",
    "    elif isinstance(msg, AIMessage):\n",
    "\n",
    "        result = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": msg.content\n",
    "        }\n",
    "        \n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls and len(msg.tool_calls) > 0 and not msg.tool_calls[0].get(\"name\").startswith(\"functions.\"):\n",
    "            result[\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"id\": tc[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc[\"name\"].replace(\"functions.\", \"\"),\n",
    "                        \"arguments\": json.dumps(tc[\"args\"])\n",
    "                    }\n",
    "                }\n",
    "                for tc in msg.tool_calls\n",
    "            ]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    elif isinstance(msg, ToolMessage):\n",
    "\n",
    "        return {\"role\": \"tool\", \"content\": msg.content, \"tool_call_id\": msg.tool_call_id}\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return {\"role\": \"user\", \"content\": str(msg)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
