{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ec86b6",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data preparation\n",
    "import pandas as pd\n",
    "\n",
    "df_items = pd.read_json(\n",
    "    \"../data/meta_Electronics_2022_2023_with_category_ratings_100_sample_1000.jsonl\", \n",
    "    lines=True\n",
    ")\n",
    "\n",
    "def preprocess_data(row):\n",
    "    return f\"{row['title']} {' '.join(row['features'])}\"\n",
    "    \n",
    "df_items[\"preprocessed_data\"] = df_items.apply(preprocess_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595df916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional data to the collection\n",
    "def extract_first_large_image(row):\n",
    "    return row['images'][0].get('large', None)\n",
    "\n",
    "df_items[\"first_large_image\"] = df_items.apply(extract_first_large_image, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73857927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_items.sample(n=50, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394b688",
   "metadata": {},
   "source": [
    "### Upload to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584634f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PayloadSchemaType\n",
    "\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba95e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant_client.delete_collection(collection_name=\"Amazon-items-collection-01\")\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=\"Amazon-items-collection-02-hybrid\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index to the collection\n",
    "# We will be adding index on this field, so we could apply exact search on it\n",
    "\n",
    "qdrant_client.create_payload_index(\n",
    "    collection_name=\"Amazon-items-collection-02-hybrid\",\n",
    "    field_name=\"text\",\n",
    "    field_type=PayloadSchemaType.TEXT, # we tell it that this is a text field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data to embedd\n",
    "\n",
    "# Columns we will want to write to db; instead of list we will use a dictionary so it would write nicely\n",
    "data_to_embed = df_sample[[\n",
    "    \"preprocessed_data\", \"first_large_image\", \n",
    "    \"rating_number\",\"price\", \"average_rating\"\n",
    "    ]].to_dict(orient=\"records\")\n",
    "\n",
    "data_to_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a428ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "# Create poinstructs\n",
    "pointstructs = []\n",
    "for i, data in enumerate(data_to_embed):\n",
    "    embedding = get_embedding(data[\"preprocessed_data\"]) # embed the column\n",
    "    pointstructs.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=embedding,\n",
    "            payload={\n",
    "                \"text\": data[\"preprocessed_data\"], # we can run context search on embedding (vector) + we indexed so we can use exact search on it\n",
    "                \"first_large_image\": data[\"first_large_image\"],\n",
    "                \"rating_number\": data[\"rating_number\"],\n",
    "                \"price\": data[\"price\"],\n",
    "                \"average_rating\": data[\"average_rating\"],\n",
    "            },\n",
    "        )\n",
    "    )   \n",
    "pointstructs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to db\n",
    "qdrant_client.upsert(\n",
    "    collection_name=\"Amazon-items-collection-02-hybrid\",\n",
    "    wait=True,\n",
    "    points=pointstructs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f05da",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5618e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery\n",
    "\n",
    "def retrieve_data(query, k=5):\n",
    "    \n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-02-hybrid\",\n",
    "        prefetch = [ # will return no more than 20 items from each prefetch (40 in total)\n",
    "            Prefetch(\n",
    "                query = query_embedding,\n",
    "                limit = 20), # regular similarity search (dense, similarity can be calculated)\n",
    "            Prefetch(\n",
    "                filter = Filter(must = [FieldCondition(key = \"text\",match = MatchText(text=query))]), # exact keyword search (can be another method, but indexing needs to be matching)\n",
    "                limit = 20), # sparse, keyword search (many 0s, may not find 20, can be less than 20 items)\n",
    "        ],\n",
    "        query=FusionQuery(fusion='rrf'), # rrf = reranker; \n",
    "        limit = 5, # will rerank and return top 5 items\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f96efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_data(\"earphones\").points\n",
    "\n",
    "# Scores are reranker scores (can not be compared to interim scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71a6f2",
   "metadata": {},
   "source": [
    "### Structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic models for output model (json schema)\n",
    "# instructor to wrap llms calls and ensure output structure\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eafac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for output model (json schema)\n",
    "\n",
    "class RAGGenerationResponse(BaseModel):\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa80e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run llms call using instructor\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant.\n",
    "Return an answer to the question.\n",
    "Question: What is your name?\n",
    "\"\"\"\n",
    "\n",
    "response, raw_response = client.chat.completions.create_with_completion(\n",
    "    model = \"gpt-4.1\",\n",
    "    response_model = RAGGenerationResponse,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature = 0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
